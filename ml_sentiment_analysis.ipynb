{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Machine Learning To Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the IMDb movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyprind\n",
    "import os\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code snippet for reading multiple text documents from diffenrent resources and joining them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basepath = '../../../aclImdb/'\n",
    "\n",
    "labels = {'pos':1, 'neg':0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test','train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath,s,l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path,file), 'r') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# suffle the DataFrame\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the new csv file\n",
    "df.to_csv('./movie_data1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've watched a lot of television in my 51 years, but I've never had so much fun week after week, as I had watching Oz. The acting by the entire cast was excellent. The writing was just perfect, with every character remaining consistent throughout the six year run. I also enjoyed the mayhem and the ultra-violence. It may sound odd, but it was at times, comical finding out how one of the characters would eventually end up dead. I particularly enjoyed the true romance and love between Beecher and Keller. Those two men really knew how to throw down, in every way possible. I truly hope that HBO will continue to show us re-runs of this great show FOREVER! I've watched every episode at least 4 times yet I still look forward to Tuesday and Thursday nights at 11 p.m. for an episode of this fun and very entertaining show.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../movie_data.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# Introducing the bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming documents into feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'\n",
    "    ])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'and': 0,\n",
       " u'is': 1,\n",
       " u'one': 2,\n",
       " u'shining': 3,\n",
       " u'sun': 4,\n",
       " u'sweet': 5,\n",
       " u'the': 6,\n",
       " u'two': 7,\n",
       " u'weather': 8}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw term frequencies: tf (t,d)â€”the number of times a term t occurs in a document d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [2, 3, 2, 1, 1, 1, 2, 1, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw term frequencies\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing word relevancy via term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
    "\n",
    "\n",
    "$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.    0.56  0.56  0.    0.43  0.    0.  ]\n",
      " [ 0.    0.43  0.    0.    0.    0.56  0.43  0.    0.56]\n",
      " [ 0.5   0.45  0.5   0.19  0.19  0.19  0.3   0.25  0.19]]\n"
     ]
    }
   ],
   "source": [
    "# TfidfTransformer, that takes the raw term frequencies from CountVectorizer as input and transforms them into tf-idfs\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "print(tfidf.fit_transform(bag).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations for the idf and tf-idf that were implemented in scikit-learn are:\n",
    "\n",
    "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
    "\n",
    "The tf-idf equation that was implemented in scikit-learn is as follows:\n",
    "\n",
    "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
    "\n",
    "While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the `TfidfTransformer` normalizes the tf-idfs directly.\n",
    "\n",
    "By default (`norm='l2'`), scikit-learn's TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector *v* by its L2-norm:\n",
    "\n",
    "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate tf-idf for \"is\"\n",
    "tf_is = 3\n",
    "n_docs = 3\n",
    "idf_is = np.log((n_docs+1)/(3+1))\n",
    "tfidf_is = tf_is * (idf_is+1)\n",
    "tfidf_is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.39,  3.  ,  3.39,  1.29,  1.29,  1.29,  2.  ,  1.69,  1.29])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf vector\n",
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_input = tfidf.fit_transform(bag).toarray()[-1]\n",
    "raw_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$\n",
    "\n",
    "$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n",
    "\n",
    "$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5 ,  0.45,  0.5 ,  0.19,  0.19,  0.19,  0.3 ,  0.25,  0.19])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l2 normalization\n",
    "l2_tfidf = raw_input/np.sqrt(np.sum(raw_input**2))\n",
    "l2_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data includes html markups that needs to be cleaned\n",
    "df.loc[0,'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0,'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing documents into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'runner', u'like', u'runnign', u'so', u'they', u'run']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 'runners like runnign so they run'\n",
    "tokenizer_porter(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'runnign', 'so', 'they', 'run']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/samet/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'runner', u'like', u'run', u'run', u'lot']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and run a lot') if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# Training a logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000,'review'].values\n",
    "y_train = df.loc[:25000,'sentiment'].values\n",
    "X_test = df.loc[25000:,'review'].values\n",
    "y_test = df.loc[25000:,'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                       lowercase=None,\n",
    "                       preprocessor=None)\n",
    "# try with/witout stopwords\n",
    "# second dictionary is without idf and normalization to train a model with raw frequencies\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "              'vect__stop_words': [stop, None],\n",
    "              'vect__tokenizer': [tokenizer,tokenizer_porter],\n",
    "              'clf__penalty': ['l1','l2'],\n",
    "              'clf__C' : [1.0, 10.0, 100.0]},\n",
    "             {'vect__ngram_range': [(1,1)],\n",
    "              'vect__stop_words': [stop, None],\n",
    "              'vect__tokenizer': [tokenizer,tokenizer_porter],\n",
    "              'vect__use_idf': [False],\n",
    "              'vect__norm': [None],\n",
    "              'clf__penalty': ['l1','l2'],\n",
    "              'clf__C': [1.0, 10.0, 100.0]}]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                   ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                          scoring='accuracy',\n",
    "                          cv=5,\n",
    "                          verbose=1,\n",
    "                          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes 40 min to train!\n",
    "# gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    > print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "    > print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)\n",
    "\n",
    "    Best parameter set: {'vect__tokenizer': <function tokenizer at 0x11851c6a8>, 'clf__C': 10.0, 'vect__stop_words': None, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1)} \n",
    "    CV Accuracy: 0.897\n",
    "\n",
    "\n",
    "    > clf = gs_lr_tfidf.best_estimator_\n",
    "    > print('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "\n",
    "    Test Accuracy: 0.899"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustrating gs_lr_tfidf.best_score_ is the average k-fold cross-validation score. \n",
    "If we have a GridSearchCV object with 5-fold cross-validation (like the one above), the best_score_ attribute returns the average score over the 5-folds of the best model. To illustrate this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f6a10ee1210>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqWMMfknqGINfkjrG4JekjjH4Jalj/j90sdGVJWb0jQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a10fd56d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a toy dataset\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=6)\n",
    "y = [np.random.randint(3) for i in range(25)]\n",
    "X = (y + np.random.randn(25)).reshape(-1,1)\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6,  0.4,  0.6,  0.2,  0.6])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False, random_state=0).split(X,y))\n",
    "# calculate the 5 accuracy values for the 5 test folds.\n",
    "cross_val_score(LogisticRegression(random_state=123), X, y, cv=cv5_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.600000, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.400000, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.600000, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.200000, total=   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.600000, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# The scores for the 5 folds are exactly the same as the ones from cross_val_score earlier.\n",
    "gs = GridSearchCV(LogisticRegression(), {}, cv=cv5_idx, verbose=3).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47999999999999998"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result above is consistent with the average score computed the cross_val_score.\n",
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47999999999999998"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LogisticRegression(random_state=123), X, y, cv=cv5_idx).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# Working with bigger data - online algorithms and out-of-core learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "#### Note: Generators\n",
    "\n",
    "Generators are iterators, but you can only iterate over them once. It's because they do not store all the values in memory, they generate the values on the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an iterable list comprehension\n",
    "[x*x for x in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7f6a10e253c0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an iterable generator\n",
    "(x*x for x in range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "mygenerator = (x*x for x in range(3))\n",
    "for i in mygenerator:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cannot perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in mygenerator:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yield is a keyword that is used like return, except the function will return a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object createGenerator at 0x7f6a10e25780>\n"
     ]
    }
   ],
   "source": [
    "def createGenerator():\n",
    "    mylist = range(3)\n",
    "    for i in mylist:\n",
    "        yield i*i\n",
    "        \n",
    "mygenerator = createGenerator()\n",
    "# mygenerator is an object\n",
    "print mygenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in mygenerator:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in mygenerator:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = stream_docs('../movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((),\n",
       " '\"OK... so... I really like Kris Kristofferson and his usual easy going delivery of lines in his movies. Age has helped him with his soft spoken low energy style and he will steal a scene effortlessly. But, Disappearance is his misstep. Holy Moly, this was a bad movie! <br /><br />I must give kudos to the cinematography and and the actors, including Kris, for trying their darndest to make sense from this goofy, confusing story! None of it made sense and Kris probably didn\\'t understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about! <br /><br />I don\\'t care that everyone on this movie was doing out of love for the project, or some such nonsense... I\\'ve seen low budget movies that had a plot for goodness sake! This had none, zilcho, nada, zippo, empty of reason... a complete waste of good talent, scenery and celluloid! <br /><br />I rented this piece of garbage for a buck, and I want my money back! I want my 2 hours back I invested on this Grade F waste of my time! Don\\'t watch this movie, or waste 1 minute of your valuable time while passing through a room where it\\'s playing or even open up the case that is holding the DVD! Believe me, you\\'ll thank me for the advice!\"')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)[:-3], next(iterator)[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates small sized training sets for stochastic gradient desc\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                        n_features=2**21,\n",
    "                        preprocessor=None,\n",
    "                        tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "\n",
    "doc_stream = stream_docs(path='../movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:06\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86699999999999999"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.883\n"
     ]
    }
   ],
   "source": [
    "print 'Accuracy: %0.3f' %clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the last 5000 documents to update the model\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
